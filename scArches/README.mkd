# HRCA v1.0 reference model using scArches

This tutorial aims to demonstrate the training of the reference model for the HRCA v1.0 using [scArches](https://docs.scarches.org/en/latest/index.html), and cell type annotation using the trained reference model. The utilities in this tutorial refer to the source code used in the [HLCA tutorial](https://docs.scarches.org/en/latest/hlca_map_classify.html). Please refer to [Li et al. 2023](https://doi.org/10.1101/2023.11.07.566105) for detailed description of HRCA.

## Installation

```Bash
$ mamba create -n scarches python=3.10 'scarches>=0.6.1' pytorch-gpu 'chex>=0.1.87' 'jax>=0.4.31' 'jaxlib>=0.4.31' 'scvi-tools>=1.2.0' 'scanpy>=1.10.3'
```

It is recommended to install scArches via [Conda](https://docs.conda.io/en/latest/miniconda.html) or [Mamba](https://anaconda.org/conda-forge/mamba). The command above has been tested on a GPU machine. Dependency versions are specified as of October 2024. An example environment file is provided as ["scarches.yaml"](./scarches.yaml).

To train the reference model and predict cell types using the trained reference model, two utilities, "scarchesh5ad2refbyscanvi" and "scarches2queryannobyscanvi", have been encapsulated in a [HRCAutils](https://github.com/lijinbio/HRCAutils) Python package and they can be installed through the following command lines.

```Bash
$ conda activate scarches
$ pip install git+https://github.com/lijinbio/HRCAutils
```

## Train HRCA v1.0 reference model (Optional)

This step is optional in this tutorial, but used to train the reference model for HRCA v1.0 using the customized utility “scarchesh5ad2refbyscanvi”. This utility has encapsulated the command lines to train the reference model using scArches. To train the reference model, the raw count matrix of HRCA v1.0 can be downloaded from Zenodo at https://doi.org/10.5281/zenodo.14014720, named “HRCA_snRNA_allcells_rawcounts.h5ad”. This can be done using the [gdown Python package](https://anaconda.org/conda-forge/gdown). The input cell-by-gene count matrix file should include raw counts without normalization. The gene symbols are used as gene features during model training. The following is example code for training a reference model for HRCA v1.0.

```Bash

$ infile=HRCA_snRNA_allcells_rawcounts.h5ad
$ outdir=scarchesh5ad2refbyscanvi
$ bname=HRCA_snRNA_allcells_rawcounts

# Download reference .h5ad file
$ gdown https://zenodo.org/records/14014720/files/HRCA_snRNA_allcells_rawcounts.h5ad

$ scarchesh5ad2refbyscanvi -d "$outdir" -b "$bname" -e scarches -k sampleid -l celltype -n 10000 -- "$infile"
```

The following is the detailed description from the help page of the “scarchesh5ad2refbyscanvi” utility.

```Bash
$ scarchesh5ad2refbyscanvi -h
Usage: scarchesh5ad2refbyscanvi [OPTIONS] INFILE
  To build a scArches reference using the scANVI model.
  INFILE is a .h5ad file.
  Example:
    f=HRCA_snRNA_allcells.h5ad
    bname=$(basename "$f" .h5ad)
    outdir=$(mrrdir.sh)
    scarchesh5ad2refbyscanvi -d "$outdir" -b "$bname" -e scarches -k sampleid -l celltype -n 10000 -- "$f"
  Note:
    1. A scANVI model might be always superior to a scVI model. So, use the scANVI model for now. (TODO: scVI model)
    2. This is forced to run on a GPU machine, see "-g|--gpu".
  See also:
    Depends:
      Python/scarches
      Python/scanpy
      Python/scvi-tools
  Date: 2024/10/16
  Authors: Jin Li <lijin.abc@gmail.com>
          
Options:
  -d, --outdir PATH               Outdir.  [default: .]
  -b, --bname TEXT                Bname.  [required]
  -e, --condaenv TEXT             Conda environment.
  -s, --seed INTEGER              Random seed.  [default: 12345]
  -k, --batchkey TEXT             Batch key. E.g., sampleid.  [required]
  -l, --label TEXT                Cell type label.  [required]
  -n, --nhvg INTEGER              Number of HVGs.  [default: 10000]
  -f, --flavor [seurat|cell_ranger|seurat_v3]
                                  HVG algorithm.  [default: seurat]
  -r, --nlayer INTEGER            Number of hidden layers used for encoder and
                                  decoder NNs.  [default: 2]
  -t, --nlatent INTEGER           Dimensionality of the latent space.
                                  [default: 30]
  -p, --epoch INTEGER             Max epoches for training.
  -g, --gpu INTEGER               A GPU device.  [default: 0]
  -h, --help                      Show this message and exit.
```

Given the big number of over 3.1 millon cells, the utility running took over 3 hours using around 350GB memory on a GPU machine. Due to the extensive computation resources and running time, this reference model training step is optional, and the trained model can be directly used in query cell projection and cell type annotation.

The following two UMAPs display the embeddings of the full 3.1 million cells in the trained reference model. The 10 major classes and 123 cell types from the reference are impressively well-isolated in the visualization.

| HRCA v1.0: majorclass | HRCA v1.0: cell type |
|---------|---------|
| <img src="figures/umapHRCA_snRNA_allcells_rawcounts_umap_majorclass_wolabel.png" alt="HRCAv1.0 UMAP with majorclass labels" width="300"> | <img src="figures/umapHRCA_snRNA_allcells_rawcounts_umap_celltype_wolabel.png" alt="HRCAv1.0 UMAP with cell type labels" width="300"> |

In the reference model, the contingency table on the left shows example cell type labels, comparing self-predicted labels with input labels. High self-prediction metrics, such as a 99.41% F1 score, demonstrate the consistent classification accuracy of the trained reference model.

| HRCA Reference Model: contingency table | self-prediction metrics |
|---------|---------|
| <img src="figures/HRCA_scArches_QC_tab.png" alt="HRCA Reference Model: contingency table" width="450"> | <img src="figures/HRCA_reference_metric.png" alt="HRCA Reference model: self-prediction metrics" width="150"> |

The trained reference model is available in Zenodo at https://doi.org/10.5281/zenodo.14014720, named “model.pt”. This reference model file can be directly used in the cell type prediction for a query dataset. More details about cell type prediction can be found in the next section.

## Annotate cell types using trained reference model

Utilizing the reference model file “model.pt”, the customized utility “scarches2queryannobyscanvi” can be used to predict cell type labels for a query dataset efficiently. The following is example code for predicting cell type labels using the trained reference model.

```Bash
$ reference=scarchesh5ad2refbyscanvi/HRCA_snRNA_allcells_rawcounts_model_scanvi
$ infile=query.h5ad
$ outdir=scarches2queryannobyscanvi

# Download the reference model if not generated
$ gdown -O “$reference/” https://zenodo.org/records/14014720/files/model.pt

# Download query .h5ad file
$ gdown https://zenodo.org/records/14014720/files/query.h5ad

$ scarches2queryannobyscanvi -d "$outdir" -b query -e scarches -r "$reference" -- "$infile"
```

```
Warning: sampleid used in reference is not in query. So, assume one single sample for query.
INFO     File                                                                   
         scarchesh5ad2refbyscanvi/HRCA_snRNA_allcells_rawcounts_model_scanvi/model.pt       
         already downloaded                                                     
INFO     Found 92.7% reference vars in query data.                              
INFO     File                                                                   
         scarchesh5ad2refbyscanvi/HRCA_snRNA_allcells_rawcounts_model_scanvi/model.pt       
         already downloaded                                                     
Info: surgerymodel, labeled indices, 0
Info: surgerymodel, unlabeled indices, 11531
INFO     Training for 400 epochs.                                               
Epoch 400/400: 100%|██████████| 400/400 [10:03<00:00,  1.51s/it, v_num=1, train_loss_step=3.63e+3, train_loss_epoch=3.67e+3]
```

A detailed help page for the “scarches2queryannobyscanvi” utility can be found below.

```Bash
$ scarches2queryannobyscanvi -h
Usage: scarches2queryannobyscanvi [OPTIONS] INFILE
  To predict cell type label against a pre-trained scArches reference using
  the scANVI model.
  INFILE is a query .h5ad file.
  Example:
    reference=HRCA_snRNA_allcells_model_scanvi
    infile=Retina_sample1.h5ad
    bname=$(basename "$infile" .h5ad)
    outdir=$(mrrdir.sh)
    scarches2queryannobyscanvi -d "$outdir" -b "$bname" -e scarches -r "$reference" -k sampleid -- "$infile"
  Note:
    1. The batch key (by "-k|--batchkey") is for query, and it might be different from the batch_key used in the reference. Internally, the reference batch_key will be added to the query.
  See also:
    Upstream:
      scarchesh5ad2refbyscanvi
    Depends:
      Python/scarches
      Python/scanpy
      Python/scvi-tools
  Date: 2024/10/30
  Authors: Jin Li <lijin.abc@gmail.com>
          
Options:
  -d, --outdir PATH     Outdir.  [default: .]
  -b, --bname TEXT      Bname.  [required]
  -e, --condaenv TEXT   Conda environment.
  -r, --reference PATH  A pre-trained model directory for reference.
                        [required]
  -k, --batchkey TEXT   Batch key in query. E.g., sampleid.
  -s, --seed INTEGER    Random seed.  [default: 12345]
  -p, --epoch INTEGER   Max epoches for training.
  -g, --gpu INTEGER     A GPU device.  [default: 0]
  -h, --help            Show this message and exit.
```

Running this query data was efficient on a GPU machine with limited computing resources, needing less than 500MB of memory and under 4 minutes of processing time. The isolated clusters in the query data embedding and high concordance between annotated and predicted major class labels demonstrated accurate label transfer with the HRCA v1.0 reference model.

| Query: majorclass | Query: label prediction | Query: contingency table |
|---------|---------|---------|
| <img src="figures/umapquery_query_umap_majorclass_wolabel.png" alt="Query: majorclass" width="200"> | <img src="figures/umapquery_query_umap_scANVI_predictions_wolabel.png" alt="Query: label prediction" width="200"> | <img src="figures/HRCA_scArches_query_map.png" alt="Query: contingency table" width="200"> |

